{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Importing all the necessary libraries\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import xml.etree.ElementTree as ET\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv2D, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "nQIiCq26dNQk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Assigning the paths of the directories\n",
        "\n",
        "DATASET_PATH = '/content/drive/MyDrive/Annotated-PascalVOCxml'\n",
        "ANNOTATIONS_PATH = '/content/drive/MyDrive/Annotated-PascalVOCxml/Annotations'\n",
        "IMAGES_PATH = '/content/drive/MyDrive/Annotated-PascalVOCxml/PNGImages'\n",
        "MODEL_PATH = \"/content/drive/MyDrive/ssd_model_voc-Generated.h5\""
      ],
      "metadata": {
        "id": "3mgScaaydUH1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = (300, 300)\n",
        "BATCH_SIZE = 8\n",
        "NUM_CLASSES = 5 # As I have annoted the dataset into 5 classes"
      ],
      "metadata": {
        "id": "C6jlNqn-exEz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pretrained model - MobileNetV2\n",
        "\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(300, 300, 3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWsSAsSMe_Oe",
        "outputId": "63829618-c032-4577-98dd-80180b3f9499"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-040e74ccb839>:3: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(300, 300, 3))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model feature extraction\n",
        "\n",
        "x = Flatten()(base_model.output)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)"
      ],
      "metadata": {
        "id": "xtvSmFCJfGCJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is for multi-object detection output\n",
        "\n",
        "bbox_output = Dense(NUM_CLASSES * 4, activation='linear', name=\"bounding_box\")(x)\n",
        "bbox_output = Reshape((NUM_CLASSES, 4))(bbox_output)\n"
      ],
      "metadata": {
        "id": "7fC1hJQLfR6u"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define our SSD model\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=bbox_output)\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss=\"mse\")\n"
      ],
      "metadata": {
        "id": "M3z8Rpi8fm4X"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To parse PASCAL VOC annotations\n",
        "\n",
        "def parse_voc_annotations(annotations_path, images_path):\n",
        "    \"\"\" Parses PASCAL VOC XML annotations and returns image-label pairs. \"\"\"\n",
        "    data = []\n",
        "\n",
        "    for xml_file in os.listdir(annotations_path):\n",
        "        xml_path = os.path.join(annotations_path, xml_file)\n",
        "        image_name = xml_file.replace('.xml', '')\n",
        "\n",
        "        # Find corresponding image file\n",
        "\n",
        "        image_file = None\n",
        "        for ext in ['.jpg', '.jpeg', '.png']:\n",
        "            if os.path.exists(os.path.join(images_path, image_name + ext)):\n",
        "                image_file = image_name + ext\n",
        "                break\n",
        "\n",
        "        # Skip if no matching image\n",
        "        if not image_file:\n",
        "            continue\n",
        "\n",
        "        image_path = os.path.join(images_path, image_file)\n",
        "\n",
        "        # Parse XML for bounding boxes\n",
        "\n",
        "        tree = ET.parse(xml_path)\n",
        "        root = tree.getroot()\n",
        "        objects = []\n",
        "\n",
        "        size = root.find('size')\n",
        "        img_width = int(size.find('width').text)\n",
        "        img_height = int(size.find('height').text)\n",
        "\n",
        "        for obj in root.findall('object'):\n",
        "            label = obj.find('name').text.strip().lower()\n",
        "            bbox = obj.find('bndbox')\n",
        "            xmin, ymin, xmax, ymax = (\n",
        "                int(bbox.find('xmin').text) / img_width,\n",
        "                int(bbox.find('ymin').text) / img_height,\n",
        "                int(bbox.find('xmax').text) / img_width,\n",
        "                int(bbox.find('ymax').text) / img_height\n",
        "            )\n",
        "            objects.append((label, (xmin, ymin, xmax, ymax)))\n",
        "\n",
        "        data.append((image_path, objects))\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "aFYioC3wgBsq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "\n",
        "dataset = parse_voc_annotations(ANNOTATIONS_PATH, IMAGES_PATH)\n",
        "train_data, val_data = train_test_split(dataset, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "Q7suantAhlvN"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data generator\n",
        "\n",
        "def data_generator(dataset, batch_size=8):\n",
        "    while True:\n",
        "        random.shuffle(dataset)\n",
        "        for i in range(0, len(dataset), batch_size):\n",
        "            batch = dataset[i:i + batch_size]\n",
        "            images, targets = [], []\n",
        "\n",
        "            for image_path, objects in batch:\n",
        "                img = load_img(image_path, target_size=IMAGE_SIZE)\n",
        "                img_array = img_to_array(img) / 255.0\n",
        "\n",
        "                # Encode multiple objects per image\n",
        "                bbox_target = np.zeros((NUM_CLASSES, 4))\n",
        "                for j, obj in enumerate(objects[:NUM_CLASSES]):\n",
        "                    _, (xmin, ymin, xmax, ymax) = obj\n",
        "                    bbox_target[j] = [xmin, ymin, xmax, ymax]\n",
        "\n",
        "                images.append(img_array)\n",
        "                targets.append(bbox_target)\n",
        "\n",
        "            yield np.array(images), np.array(targets)"
      ],
      "metadata": {
        "id": "FpJEo1Kzh8ic"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train SSD model\n",
        "\n",
        "history = model.fit(\n",
        "    data_generator(train_data, BATCH_SIZE),\n",
        "    steps_per_epoch=len(train_data) // BATCH_SIZE,\n",
        "    validation_data=data_generator(val_data, BATCH_SIZE),\n",
        "    validation_steps=len(val_data) // BATCH_SIZE,\n",
        "    epochs=30\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmPgJyBoiFs3",
        "outputId": "8737aa0b-f4c3-4dc3-cd02-874f398d5d43"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 5s/step - loss: 6.9473 - val_loss: 0.7945\n",
            "Epoch 2/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 5s/step - loss: 4.9876 - val_loss: 0.1838\n",
            "Epoch 3/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 5s/step - loss: 0.4153 - val_loss: 0.1768\n",
            "Epoch 4/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 5s/step - loss: 0.2287 - val_loss: 0.1572\n",
            "Epoch 5/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 10s/step - loss: 0.1721 - val_loss: 0.1596\n",
            "Epoch 6/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 5s/step - loss: 0.1473 - val_loss: 0.1459\n",
            "Epoch 7/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 5s/step - loss: 0.1401 - val_loss: 0.1754\n",
            "Epoch 8/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 5s/step - loss: 0.1416 - val_loss: 0.1376\n",
            "Epoch 9/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 5s/step - loss: 0.1274 - val_loss: 0.1648\n",
            "Epoch 10/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 6s/step - loss: 0.1260 - val_loss: 0.1258\n",
            "Epoch 11/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 5s/step - loss: 0.1298 - val_loss: 0.1298\n",
            "Epoch 12/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 10s/step - loss: 0.1208 - val_loss: 0.1379\n",
            "Epoch 13/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 5s/step - loss: 0.1175 - val_loss: 0.1089\n",
            "Epoch 14/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 5s/step - loss: 0.1084 - val_loss: 0.1066\n",
            "Epoch 15/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 5s/step - loss: 0.1000 - val_loss: 0.1199\n",
            "Epoch 16/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 5s/step - loss: 0.1063 - val_loss: 0.1013\n",
            "Epoch 17/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 6s/step - loss: 0.0944 - val_loss: 0.1059\n",
            "Epoch 18/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 6s/step - loss: 0.0982 - val_loss: 0.0998\n",
            "Epoch 19/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 10s/step - loss: 0.1008 - val_loss: 0.0956\n",
            "Epoch 20/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 6s/step - loss: 0.0953 - val_loss: 0.0900\n",
            "Epoch 21/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 5s/step - loss: 0.0905 - val_loss: 0.0992\n",
            "Epoch 22/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 5s/step - loss: 0.0961 - val_loss: 0.0811\n",
            "Epoch 23/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 5s/step - loss: 0.0923 - val_loss: 0.0897\n",
            "Epoch 24/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 6s/step - loss: 0.0965 - val_loss: 0.0868\n",
            "Epoch 25/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 6s/step - loss: 0.0915 - val_loss: 0.0796\n",
            "Epoch 26/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 5s/step - loss: 0.0869 - val_loss: 0.0802\n",
            "Epoch 27/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 6s/step - loss: 0.0824 - val_loss: 0.0717\n",
            "Epoch 28/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 5s/step - loss: 0.0757 - val_loss: 0.0808\n",
            "Epoch 29/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 5s/step - loss: 0.0762 - val_loss: 0.0737\n",
            "Epoch 30/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 6s/step - loss: 0.0914 - val_loss: 0.0734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "\n",
        "model.save(MODEL_PATH)\n",
        "print(f\"SSD Model saved at {MODEL_PATH}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DIRr1VGiPqV",
        "outputId": "6d67e3ba-f811-42c6-bc8b-e6cbae75bcca"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SSD Model saved at /content/drive/MyDrive/ssd_model_voc-Generated.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Evaluation\n",
        "\n",
        "eval_results = model.evaluate(data_generator(val_data, BATCH_SIZE), steps=len(val_data) // BATCH_SIZE)\n",
        "print(f\"Validation Loss: {eval_results}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHBhGDSKiiTn",
        "outputId": "2423fba0-a644-40f7-adcf-c067df2d547f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 609ms/step - loss: 0.0782\n",
            "Validation Loss: 0.07690971344709396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LlGKm_Oyi9Zf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}